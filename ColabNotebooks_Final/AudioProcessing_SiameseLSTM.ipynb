{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dont run this"
      ],
      "metadata": {
        "id": "vcxgESxhbr5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9toixmq8OVPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5119bee-2da6-487f-b0ad-f7dec7b4ca63"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape of anchors: (3766595, 4)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'recurrent_kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 10ms/step - loss: 0.2512 - val_loss: 7.4723 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 10ms/step - loss: 0.2008 - val_loss: 12.2373 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 9ms/step - loss: 0.1996 - val_loss: 17.7669 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 9ms/step - loss: 0.2034 - val_loss: 21.4143 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 9ms/step - loss: 0.1960 - val_loss: 26.4838 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 9ms/step - loss: 0.1995 - val_loss: 27.5801 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 9ms/step - loss: 0.1996 - val_loss: 28.2628 - learning_rate: 2.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 9ms/step - loss: 0.1985 - val_loss: 28.6985 - learning_rate: 2.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 9ms/step - loss: 0.1955 - val_loss: 29.1204 - learning_rate: 2.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 9ms/step - loss: 0.2016 - val_loss: 30.0128 - learning_rate: 2.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# with lambda function - causes a lambda error\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Dropout, Lambda\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "def split_audio_into_segments(audio_file, segment_length=10):\n",
        "    \"\"\"Splits audio file into segments of specified length.\"\"\"\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    segment_samples = segment_length * sr\n",
        "    num_segments = len(y) // segment_samples\n",
        "    segments = [y[i * segment_samples:(i + 1) * segment_samples] for i in range(num_segments)]\n",
        "    return segments, sr\n",
        "\n",
        "def extract_features(file, sr):\n",
        "    audio, _ = librosa.load(file, sr=sr)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "    return mfcc, mel, chroma, spectral_contrast\n",
        "\n",
        "def process_movies_and_trailers(movie_files, trailer_files, sr):\n",
        "    all_mfcc_movie = []\n",
        "    all_mel_movie = []\n",
        "    all_chroma_movie = []\n",
        "    all_spectral_contrast_movie = []\n",
        "\n",
        "    all_mfcc_trailer = []\n",
        "    all_mel_trailer = []\n",
        "    all_chroma_trailer = []\n",
        "    all_spectral_contrast_trailer = []\n",
        "\n",
        "    for file in movie_files:\n",
        "        mfcc, mel, chroma, spectral_contrast = extract_features(file, sr)\n",
        "        all_mfcc_movie.append(mfcc.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_mel_movie.append(mel.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_chroma_movie.append(chroma.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_spectral_contrast_movie.append(spectral_contrast.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "\n",
        "    for file in trailer_files:\n",
        "        mfcc, mel, chroma, spectral_contrast = extract_features(file, sr)\n",
        "        all_mfcc_trailer.append(mfcc.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_mel_trailer.append(mel.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_chroma_trailer.append(chroma.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_spectral_contrast_trailer.append(spectral_contrast.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "\n",
        "    return all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie, \\\n",
        "           all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer\n",
        "\n",
        "def prepare_triplet_data(all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie,\n",
        "                         all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer):\n",
        "    \"\"\"Prepares triplet data for Siamese LSTM model.\"\"\"\n",
        "    anchors, positives, labels = [], [], []\n",
        "    num_movies = len(all_mfcc_movie)\n",
        "    num_trailers = len(all_mfcc_trailer)\n",
        "\n",
        "    for i in range(num_movies):\n",
        "        min_len = min(len(all_mfcc_movie[i]), len(all_mel_movie[i]), len(all_chroma_movie[i]), len(all_spectral_contrast_movie[i]))\n",
        "        for j in range(min_len):\n",
        "            combined_anchor = np.concatenate([all_mfcc_movie[i][j], all_mel_movie[i][j], all_chroma_movie[i][j], all_spectral_contrast_movie[i][j]], axis=0)\n",
        "            trailer_idx = j % num_trailers  # Use modulo operator to wrap around\n",
        "            min_len_trailer = min(len(all_mfcc_trailer[trailer_idx]), len(all_mel_trailer[trailer_idx]), len(all_chroma_trailer[trailer_idx]), len(all_spectral_contrast_trailer[trailer_idx]))\n",
        "            combined_positive = np.concatenate([all_mfcc_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_mel_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_chroma_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_spectral_contrast_trailer[trailer_idx][j % min_len_trailer]], axis=0)\n",
        "            anchors.append(combined_anchor)\n",
        "            positives.append(combined_positive)\n",
        "            labels.append(1)  # Positive pair\n",
        "\n",
        "    for i in range(num_trailers):\n",
        "        min_len = min(len(all_mfcc_trailer[i]), len(all_mel_trailer[i]), len(all_chroma_trailer[i]), len(all_spectral_contrast_trailer[i]))\n",
        "        for j in range(min_len):\n",
        "            combined_anchor = np.concatenate([all_mfcc_trailer[i][j], all_mel_trailer[i][j], all_chroma_trailer[i][j], all_spectral_contrast_trailer[i][j]], axis=0)\n",
        "            movie_idx = j % num_movies  # Use modulo operator to wrap around\n",
        "            min_len_movie = min(len(all_mfcc_movie[movie_idx]), len(all_mel_movie[movie_idx]), len(all_chroma_movie[movie_idx]), len(all_spectral_contrast_movie[movie_idx]))\n",
        "            combined_positive = np.concatenate([all_mfcc_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_mel_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_chroma_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_spectral_contrast_movie[movie_idx][j % min_len_movie]], axis=0)\n",
        "            anchors.append(combined_anchor)\n",
        "            positives.append(combined_positive)\n",
        "            labels.append(0)  # Negative pair\n",
        "\n",
        "    anchors = np.array(anchors)\n",
        "    positives = np.array(positives)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Standardize features for better training performance\n",
        "    scaler = StandardScaler()\n",
        "    anchors = scaler.fit_transform(anchors)\n",
        "    positives = scaler.transform(positives)\n",
        "\n",
        "    return anchors, positives, labels\n",
        "\n",
        "def create_siamese_lstm_model(input_shape):\n",
        "    \"\"\"Defines the Siamese LSTM model architecture.\"\"\"\n",
        "    import tensorflow as tf\n",
        "    if len(input_shape) == 1:  # If input_shape has only one element\n",
        "        timesteps = 10  # Default number of time steps\n",
        "        features = input_shape[0]\n",
        "    else:\n",
        "        timesteps, features = input_shape\n",
        "\n",
        "    input_a = Input(shape=(timesteps, features))  # Use the correct input shape\n",
        "    input_b = Input(shape=(timesteps, features))  # Use the correct input shape\n",
        "\n",
        "    # Shared LSTM layers\n",
        "    lstm = LSTM(128, return_sequences=True)\n",
        "    lstm_a = lstm(input_a)\n",
        "    lstm_a = LSTM(64)(lstm_a)\n",
        "    dense_a = Dense(32, activation='relu')(lstm_a)\n",
        "\n",
        "    lstm_b = lstm(input_b)\n",
        "    lstm_b = LSTM(64)(lstm_b)\n",
        "    dense_b = Dense(32, activation='relu')(lstm_b)\n",
        "\n",
        "    # Calculate distance between outputs\n",
        "    distance = Lambda(lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=-1, keepdims=True)),\n",
        "                     output_shape=(1,))(dense_a, dense_b)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
        "    return model\n",
        "\n",
        "def contrastive_loss(margin=1):\n",
        "    def loss(y_true, y_pred):\n",
        "        # Add a small value to y_pred to prevent division by zero\n",
        "        y_pred = tf.maximum(y_pred, 1e-8)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = tf.reduce_mean((1 - y_true) * tf.square(y_pred) + y_true * tf.square(tf.maximum(margin - y_pred, 0)))\n",
        "\n",
        "        # Check for NaN values and replace them with zeros\n",
        "        loss = tf.where(tf.math.is_nan(loss), tf.zeros_like(loss), loss)\n",
        "\n",
        "        return loss\n",
        "    return loss\n",
        "\n",
        "# Example usage\n",
        "trailer_files = [\n",
        "        '/content/Stucco_Trailer.wav',\n",
        "        '/content/SushiNoh_Trailer.wav',\n",
        "        '/content/THECHAIR_Trailer.wav',\n",
        "        '/content/TheCouch_Trailer.wav',\n",
        "        '/content/TheElevator_Trailer.wav'\n",
        "]\n",
        "\n",
        "movie_files = [\n",
        "        '/content/Stucco _Movie.wav',\n",
        "        '/content/SushiNoh_Movie.wav',\n",
        "        '/content/THECHAIR_Movie.wav',\n",
        "        '/content/TheCouch_Movie.wav',\n",
        "        '/content/TheElevator_Movie.wav'\n",
        "]\n",
        "\n",
        "segments, sr = split_audio_into_segments(trailer_files[0])\n",
        "all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie, \\\n",
        "all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer = process_movies_and_trailers(movie_files, trailer_files, sr)\n",
        "\n",
        "anchors, positives, labels = prepare_triplet_data(all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie,\n",
        "                                                 all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer)\n",
        "# Check the original shape of the anchors array\n",
        "print(\"Original shape of anchors:\", anchors.shape)\n",
        "\n",
        "# Calculate the number of samples (batch size) and time steps\n",
        "batch_size = anchors.shape[0]\n",
        "time_steps = anchors.shape[1] // 4  # assuming 4 features\n",
        "\n",
        "# Reshape the anchors and positives arrays\n",
        "anchors = anchors.reshape(batch_size, time_steps, 4)\n",
        "positives = positives.reshape(batch_size, time_steps, 4)\n",
        "\n",
        "input_shape = anchors.shape[1:]\n",
        "model = create_siamese_lstm_model(input_shape)\n",
        "# Add dropout to prevent overfitting\n",
        "x = model.output\n",
        "x = Dropout(0.2)(x)\n",
        "model = Model(inputs=model.input, outputs=x)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001, clipvalue=0.5)\n",
        "model.compile(optimizer=optimizer, loss=contrastive_loss(margin=1))\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([anchors, positives], labels, epochs=10, batch_size=64,\n",
        "                    validation_split=0.2, callbacks=[reduce_lr, early_stopping])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model in the native Keras format\n",
        "model.save('siamese_lstm_model.h5')\n",
        "print(\"Model saved as 'siamese_lstm_model.keras'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuuIBygXmIu-",
        "outputId": "72427353-4714-42dd-e0e3-0d569d2be8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'siamese_lstm_model.keras'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRkyspgkXpYI",
        "outputId": "338fef92-8ef7-4c22-a717-20145a2af9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "sound = AudioSegment.from_mp3(\"/content/ignoreitmovieaudio.mp3\")\n",
        "sound.export(\"/content/ignoreitmovieaudio.wav\", format=\"wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM3sBZm1XiDu",
        "outputId": "632fee5b-a9c3-4d4a-eb1d-97995438db6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='/content/ignoreitmovieaudio.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "def extract_features(audio, sr):\n",
        "    \"\"\"Extracts audio features: MFCC, Mel, Chroma, Spectral Contrast.\"\"\"\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr, n_chroma=13)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "\n",
        "    # Ensure all features have the same number of frames\n",
        "    max_frames = max(mfcc.shape[1], mel.shape[1], chroma.shape[1], spectral_contrast.shape[1])\n",
        "\n",
        "    # Pad or truncate features to have the same number of frames\n",
        "    mfcc = librosa.util.fix_length(mfcc, size=max_frames, axis=1)\n",
        "    mel = librosa.util.fix_length(mel, size=max_frames, axis=1)\n",
        "    chroma = librosa.util.fix_length(chroma, size=max_frames, axis=1)\n",
        "    spectral_contrast = librosa.util.fix_length(spectral_contrast, size=max_frames, axis=1)\n",
        "\n",
        "    # Combine features into a single array and take mean across time\n",
        "    features = np.vstack([mfcc.mean(axis=0), mel.mean(axis=0), chroma.mean(axis=0), spectral_contrast.mean(axis=0)])\n",
        "    return features\n",
        "\n",
        "def pad_sequences(sequences, max_length):\n",
        "    \"\"\"Pads sequences to the same length.\"\"\"\n",
        "    padded_sequences = []\n",
        "    for sequence in sequences:\n",
        "        length = sequence.shape[1]  # Get the length of the sequence along the time dimension\n",
        "        if length < max_length:\n",
        "            pad_width = max_length - length\n",
        "            padded_sequence = np.pad(sequence, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            padded_sequence = sequence[:, :max_length]\n",
        "        padded_sequences.append(padded_sequence)\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "def get_top_trailer_worthy_segments(labels, segment_duration=5, top_n=50, movie_audio_len=0, sr=22050):\n",
        "    \"\"\"Get the top `top_n` non-overlapping trailer-worthy segments.\"\"\"\n",
        "    trailer_segments = []\n",
        "    used_indices = set()\n",
        "\n",
        "    # Calculate the total movie duration in seconds\n",
        "    total_audio_duration = movie_audio_len / sr  # Duration in seconds\n",
        "\n",
        "    # Calculate distances of each segment from the cluster centroid to rank them\n",
        "    unique_labels = np.unique(labels)\n",
        "    distances = []\n",
        "\n",
        "    for cluster in unique_labels:\n",
        "        cluster_indices = np.where(labels == cluster)[0]\n",
        "        for idx in cluster_indices:\n",
        "            # Calculate the start and end time in seconds\n",
        "            start_time = (idx * segment_duration)  # Correct scaling based on segment duration\n",
        "            end_time = start_time + segment_duration\n",
        "\n",
        "            # Ensure the segment stays within the bounds of the movie duration\n",
        "            if end_time > total_audio_duration:\n",
        "                continue  # Skip segments that go beyond the movie's duration\n",
        "\n",
        "            # Calculate distance to centroid (optional, depending on your ranking method)\n",
        "            distance_to_centroid = np.linalg.norm(labels[idx] - np.mean(labels[cluster_indices], axis=0))  # Distance to centroid\n",
        "\n",
        "            distances.append((start_time, end_time, distance_to_centroid, idx))  # store distance, time, and index\n",
        "\n",
        "    # Sort by distance (smallest to largest) - we want the closest to centroids first\n",
        "    distances.sort(key=lambda x: x[2])  # Sort by distance\n",
        "\n",
        "    # Select top_n segments and ensure non-overlapping\n",
        "    for start_time, end_time, _, idx in distances[:top_n]:\n",
        "        # Check for overlap with previously selected segments\n",
        "        if not any(start < end_time and end > start_time for start, end in trailer_segments):\n",
        "            trailer_segments.append((start_time, end_time))\n",
        "            used_indices.add(idx)\n",
        "\n",
        "    return trailer_segments\n",
        "\n",
        "def predict_timestamps(model, movie_file, trailer_files, sr):\n",
        "    \"\"\"Predicts the timestamps for the trailer-worthy segments.\"\"\"\n",
        "    # Load and extract features from the movie\n",
        "    movie_audio, _ = librosa.load(movie_file, sr=sr)\n",
        "    movie_features = extract_features(movie_audio, sr)\n",
        "\n",
        "    # Load and extract features from each trailer\n",
        "    trailer_features = []\n",
        "    for trailer_file in trailer_files:\n",
        "        trailer_audio, _ = librosa.load(trailer_file, sr=sr)\n",
        "        features = extract_features(trailer_audio, sr)\n",
        "        trailer_features.append(features)\n",
        "\n",
        "    # Pad segments to ensure uniform shape\n",
        "    max_length = max(movie_features.shape[1], max(t.shape[1] for t in trailer_features))\n",
        "    padded_movie_features = pad_sequences([movie_features], max_length)[0]\n",
        "    padded_trailer_features = pad_sequences(trailer_features, max_length)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    padded_movie_features = scaler.fit_transform(padded_movie_features.T).T\n",
        "    padded_trailer_features = np.array([scaler.transform(trailer.T).T for trailer in padded_trailer_features])\n",
        "\n",
        "    # Reshape features to fit model's expected input shape\n",
        "    def reshape_features(features):\n",
        "        \"\"\"Reshape the feature array to the shape that the model expects.\"\"\"\n",
        "        num_frames = features.shape[1]\n",
        "        if features.shape[0] != 4:\n",
        "            raise ValueError(\"Features should have a shape of (4, num_frames)\")\n",
        "\n",
        "        # Reshape the features to (num_frames, 1, 4) to match the model input shape\n",
        "        reshaped_features = features.reshape((num_frames, 1, 4))  # (num_frames, 1, 4)\n",
        "        return reshaped_features\n",
        "\n",
        "    # Reshape movie features and trailer features to fit the model input\n",
        "    movie_segments = reshape_features(padded_movie_features)\n",
        "\n",
        "    # For trailers, compute the mean across trailers and reshape\n",
        "    mean_trailer_features = np.mean(padded_trailer_features, axis=0)  # Take the mean across all trailers\n",
        "    trailer_segments = reshape_features(mean_trailer_features)  # Reshape\n",
        "\n",
        "    # Prepare inputs as a list of tensors\n",
        "    inputs = [tf.convert_to_tensor(movie_segments), tf.convert_to_tensor(trailer_segments)]\n",
        "\n",
        "    # Debugging: Print shapes and types of inputs\n",
        "    print(\"Input shapes:\")\n",
        "    print(\"Movie segments shape:\", movie_segments.shape)\n",
        "    print(\"Trailer segments shape:\", trailer_segments.shape)\n",
        "    print(\"Inputs type:\", type(inputs), \"Inputs content:\", inputs)\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(inputs)  # Pass the list of inputs\n",
        "\n",
        "    # Flatten the predicted features for clustering\n",
        "    predicted_embeddings = predictions.reshape(predictions.shape[0], -1)\n",
        "\n",
        "    # Perform K-means clustering to find trailer-worthy segments\n",
        "    num_clusters = 4  # Set this based on your criteria\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(predicted_embeddings)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Extract top trailer-worthy segments\n",
        "    movie_audio_len = len(movie_audio)  # Length of the movie audio in samples\n",
        "    trailer_segments = get_top_trailer_worthy_segments(labels, segment_duration=5, top_n=30, movie_audio_len=movie_audio_len, sr=sr)\n",
        "\n",
        "    return trailer_segments\n",
        "\n",
        "# Load the pre-trained Siamese LSTM model with safe_mode set to False\n",
        "model = load_model('/content/siamese_lstm_model.keras', custom_objects={'contrastive_loss': contrastive_loss}, compile=False, safe_mode=False)\n",
        "model.summary()\n",
        "\n",
        "# Example paths (update these with your actual paths)\n",
        "movie_file = \"/content/ignoreitmovieaudio.wav\"\n",
        "trailer_files = [\n",
        "    \"/content/Stucco_Trailer.wav\",\n",
        "    \"/content/SushiNoh_Trailer.wav\",\n",
        "    \"/content/THECHAIR_Trailer.wav\",\n",
        "    \"/content/TheElevator_Trailer.wav\",\n",
        "    \"/content/TheCouch_Trailer.wav\"\n",
        "]\n",
        "\n",
        "# Set the sample rate\n",
        "sr = 22050  # Sample rate\n",
        "\n",
        "# Predict trailer-worthy segments\n",
        "trailer_segments = predict_timestamps(model, movie_file, trailer_files, sr)\n",
        "\n",
        "# Output the results\n",
        "print(\"Top 50 non-overlapping trailer-worthy segments (start time, end time) in seconds:\")\n",
        "for segment in trailer_segments:\n",
        "    print(f\"Start: {segment[0]}s, End: {segment[1]}s\")\n",
        "\n",
        "# Optionally save the results to a CSV file\n",
        "df = pd.DataFrame(trailer_segments, columns=['Start Time (s)', 'End Time (s)'])\n",
        "df.to_csv('top_trailer_worthy_segments.csv', index=False)\n",
        "print(\"Top 30 trailer-worthy segments have been saved to 'top_trailer_worthy_segments.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "btbT7yvNWFW9",
        "outputId": "cda20300-18d3-43a1-9bd8-1caf29c5fa73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m68,096\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m49,408\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m49,408\u001b[0m │ lstm_6[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">68,096</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171,072\u001b[0m (668.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,072</span> (668.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171,072\u001b[0m (668.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,072</span> (668.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shapes:\n",
            "Movie segments shape: (16837, 1, 4)\n",
            "Trailer segments shape: (16837, 1, 4)\n",
            "Inputs type: <class 'list'> Inputs content: [<tf.Tensor: shape=(16837, 1, 4), dtype=float64, numpy=\n",
            "array([[[-3.14680743, -2.14150866, -1.77313273, -1.60382616]],\n",
            "\n",
            "       [[-1.50869514, -1.70686195, -1.49884773, -1.45279835]],\n",
            "\n",
            "       [[-1.50186953, -1.48460835, -1.38992496, -1.12190749]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.80887846, -5.14206966, -6.69890218, -4.4876505 ]],\n",
            "\n",
            "       [[-4.19442842, -2.978377  , -2.68481344, -4.92212311]],\n",
            "\n",
            "       [[-1.89791841, -2.4179231 , -7.56592033, -9.46191723]]])>, <tf.Tensor: shape=(16837, 1, 4), dtype=float64, numpy=\n",
            "array([[[ -4.41496924,  -4.26898948,  -4.40948542,  -4.94617964]],\n",
            "\n",
            "       [[ -4.94672546,  -4.94672546,  -4.94672546,  -4.94672546]],\n",
            "\n",
            "       [[ -4.94672546,  -4.94672546,  -4.56481188,  -4.27240532]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-10.98854859, -10.98854859, -10.98854859, -10.98854859]],\n",
            "\n",
            "       [[-10.98854859, -10.98854859, -10.98854859, -10.98854859]],\n",
            "\n",
            "       [[-10.98854859, -10.98854859, -10.98854859, -10.98854859]]])>]\n",
            "\u001b[1m527/527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Top 50 non-overlapping trailer-worthy segments (start time, end time) in seconds:\n",
            "Start: 0s, End: 5s\n",
            "Start: 5s, End: 10s\n",
            "Start: 10s, End: 15s\n",
            "Start: 15s, End: 20s\n",
            "Start: 20s, End: 25s\n",
            "Start: 25s, End: 30s\n",
            "Start: 30s, End: 35s\n",
            "Start: 35s, End: 40s\n",
            "Start: 40s, End: 45s\n",
            "Start: 45s, End: 50s\n",
            "Start: 50s, End: 55s\n",
            "Start: 55s, End: 60s\n",
            "Start: 60s, End: 65s\n",
            "Start: 65s, End: 70s\n",
            "Start: 70s, End: 75s\n",
            "Start: 75s, End: 80s\n",
            "Start: 80s, End: 85s\n",
            "Start: 85s, End: 90s\n",
            "Start: 90s, End: 95s\n",
            "Start: 95s, End: 100s\n",
            "Start: 100s, End: 105s\n",
            "Start: 105s, End: 110s\n",
            "Start: 110s, End: 115s\n",
            "Start: 115s, End: 120s\n",
            "Start: 120s, End: 125s\n",
            "Start: 125s, End: 130s\n",
            "Start: 130s, End: 135s\n",
            "Start: 135s, End: 140s\n",
            "Start: 140s, End: 145s\n",
            "Start: 145s, End: 150s\n",
            "Top 30 trailer-worthy segments have been saved to 'top_trailer_worthy_segments.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run for all movies !!"
      ],
      "metadata": {
        "id": "lMfUGYLna5et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "def extract_features(audio, sr):\n",
        "    \"\"\"Extracts audio features: MFCC, Mel, Chroma, Spectral Contrast.\"\"\"\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr, n_chroma=13)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "\n",
        "    # Ensure all features have the same number of frames\n",
        "    max_frames = max(mfcc.shape[1], mel.shape[1], chroma.shape[1], spectral_contrast.shape[1])\n",
        "\n",
        "    # Pad or truncate features to have the same number of frames\n",
        "    mfcc = librosa.util.fix_length(mfcc, size=max_frames, axis=1)\n",
        "    mel = librosa.util.fix_length(mel, size=max_frames, axis=1)\n",
        "    chroma = librosa.util.fix_length(chroma, size=max_frames, axis=1)\n",
        "    spectral_contrast = librosa.util.fix_length(spectral_contrast, size=max_frames, axis=1)\n",
        "\n",
        "    # Combine features into a single array and take mean across time\n",
        "    features = np.vstack([mfcc.mean(axis=0), mel.mean(axis=0), chroma.mean(axis=0),\n",
        "                          spectral_contrast.mean(axis=0)])\n",
        "    return features\n",
        "\n",
        "def pad_sequences(sequences, max_length):\n",
        "    \"\"\"Pads sequences to the same length.\"\"\"\n",
        "    padded_sequences = []\n",
        "    for sequence in sequences:\n",
        "        length = sequence.shape[1]  # Get the length of the sequence along the time dimension\n",
        "        if length < max_length:\n",
        "            pad_width = max_length - length\n",
        "            padded_sequence = np.pad(sequence, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            padded_sequence = sequence[:, :max_length]\n",
        "        padded_sequences.append(padded_sequence)\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "def get_key_audio_segments(labels, features, segment_duration=5, movie_audio_len=0,\n",
        "                           sr=22050, threshold_distance=0.1):\n",
        "    \"\"\"Filters and returns key audio moments from the clustered segments.\"\"\"\n",
        "    key_segments = []\n",
        "    total_audio_duration = movie_audio_len / sr  # Duration in seconds\n",
        "\n",
        "    # Iterate over the labels to extract all segments and their distances to the centroid\n",
        "    unique_labels = np.unique(labels)\n",
        "    for cluster in unique_labels:\n",
        "        cluster_indices = np.where(labels == cluster)[0]\n",
        "        cluster_features = features[cluster_indices]\n",
        "\n",
        "        # Calculate the centroid of the cluster\n",
        "        centroid = np.mean(cluster_features, axis=0)\n",
        "\n",
        "        for idx in cluster_indices:\n",
        "            # Calculate the start and end time of the segment\n",
        "            start_time = idx * segment_duration\n",
        "            end_time = start_time + segment_duration\n",
        "\n",
        "            # Ensure the segment stays within the bounds of the movie duration\n",
        "            if end_time > total_audio_duration:\n",
        "                continue  # Skip segments that go beyond the movie's duration\n",
        "\n",
        "            # Calculate the distance of the segment's features to the cluster centroid\n",
        "            distance_to_centroid = np.linalg.norm(cluster_features[idx] - centroid)\n",
        "\n",
        "            # Filter based on distance (closer to centroid = more likely to be a key moment)\n",
        "            if distance_to_centroid < threshold_distance:\n",
        "                key_segments.append((start_time, end_time))\n",
        "\n",
        "    return key_segments\n",
        "\n",
        "def predict_timestamps(model, movie_file, trailer_files, sr, threshold_distance=0.1):\n",
        "    \"\"\"Predicts the timestamps for the key audio moments.\"\"\"\n",
        "    # Load and extract features from the movie\n",
        "    movie_audio, _ = librosa.load(movie_file, sr=sr)\n",
        "    movie_features = extract_features(movie_audio, sr)\n",
        "\n",
        "    # Load and extract features from each trailer\n",
        "    trailer_features = []\n",
        "    for trailer_file in trailer_files:\n",
        "        trailer_audio, _ = librosa.load(trailer_file, sr=sr)\n",
        "        features = extract_features(trailer_audio, sr)\n",
        "        trailer_features.append(features)\n",
        "\n",
        "    # Pad segments to ensure uniform shape\n",
        "    max_length = max(movie_features.shape[1], max(t.shape[1] for t in trailer_features))\n",
        "    padded_movie_features = pad_sequences([movie_features], max_length)[0]\n",
        "    padded_trailer_features = pad_sequences(trailer_features, max_length)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    padded_movie_features = scaler.fit_transform(padded_movie_features.T).T\n",
        "    padded_trailer_features = np.array([scaler.transform(trailer.T).T for trailer in\n",
        "                                        padded_trailer_features])\n",
        "\n",
        "    # Reshape features to fit model's expected input shape\n",
        "    def reshape_features(features):\n",
        "        \"\"\"Reshape the feature array to the shape that the model expects.\"\"\"\n",
        "        num_frames = features.shape[1]\n",
        "        if features.shape[0] != 4:\n",
        "            raise ValueError(\"Features should have a shape of (4, num_frames)\")\n",
        "\n",
        "        # Reshape the features to (num_frames, 1, 4) to match the model input shape\n",
        "        reshaped_features = features.reshape((num_frames, 1, 4))  # (num_frames, 1, 4)\n",
        "        return reshaped_features\n",
        "\n",
        "    # Reshape movie features and trailer features to fit the model input\n",
        "    movie_segments = reshape_features(padded_movie_features)\n",
        "\n",
        "    # For trailers, compute the mean across trailers and reshape\n",
        "    mean_trailer_features = np.mean(padded_trailer_features, axis=0)  # Take the mean across all trailers\n",
        "    trailer_segments = reshape_features(mean_trailer_features)  # Reshape\n",
        "\n",
        "    # Prepare inputs as a list of tensors\n",
        "    inputs = [tf.convert_to_tensor(movie_segments), tf.convert_to_tensor(trailer_segments)]\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(inputs)  # Pass the list of inputs\n",
        "\n",
        "    # Flatten the predicted features for clustering\n",
        "    predicted_embeddings = predictions.reshape(predictions.shape[0], -1)\n",
        "\n",
        "    # Perform K-means clustering to find trailer-worthy segments\n",
        "    num_clusters = 4  # Set this based on your criteria\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(predicted_embeddings)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Extract key audio segments (those closest to cluster centroids)\n",
        "    movie_audio_len = len(movie_audio)  # Length of the movie audio in samples\n",
        "    key_segments = get_key_audio_segments(labels, predicted_embeddings, segment_duration=5,\n",
        "                    movie_audio_len=movie_audio_len, sr=sr, threshold_distance=threshold_distance)\n",
        "\n",
        "    return key_segments\n",
        "\n",
        "# Load the pre-trained Siamese LSTM model with safe_mode set to False\n",
        "model = load_model('/content/siamese_lstm_model.keras',\n",
        "                   custom_objects={'contrastive_loss': contrastive_loss},\n",
        "                   compile=False, safe_mode=False)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Set the sample rate\n",
        "sr = 22050  # Sample rate\n",
        "\n",
        "# Predict key audio moments (segments)\n",
        "key_segments = predict_timestamps(model, movie_file, trailer_files, sr, threshold_distance=0.1)\n",
        "\n",
        "\n",
        "# Optionally save the results to a CSV file\n",
        "df = pd.DataFrame(key_segments, columns=['Start Time (s)', 'End Time (s)'])\n",
        "df.to_csv('key_audio_moments.csv', index=False)\n",
        "print(\"Key audio moments have been saved to 'key_audio_moments.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "2X_MjKg-Y9PI",
        "outputId": "7029578b-b6bc-4a93-b031-7bb4e1c1802b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m68,096\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m49,408\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m49,408\u001b[0m │ lstm_6[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">68,096</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171,072\u001b[0m (668.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,072</span> (668.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171,072\u001b[0m (668.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,072</span> (668.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m527/527\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step\n",
            "Key audio moments (start time, end time) in seconds:\n",
            "Start: 100s, End: 105s\n",
            "Start: 115s, End: 120s\n",
            "Start: 130s, End: 135s\n",
            "Start: 140s, End: 145s\n",
            "Start: 150s, End: 155s\n",
            "Start: 155s, End: 160s\n",
            "Start: 270s, End: 275s\n",
            "Start: 275s, End: 280s\n",
            "Start: 285s, End: 290s\n",
            "Start: 290s, End: 295s\n",
            "Start: 295s, End: 300s\n",
            "Start: 300s, End: 305s\n",
            "Start: 310s, End: 315s\n",
            "Start: 315s, End: 320s\n",
            "Start: 320s, End: 325s\n",
            "Start: 325s, End: 330s\n",
            "Start: 345s, End: 350s\n",
            "Start: 350s, End: 355s\n",
            "Start: 360s, End: 365s\n",
            "Start: 370s, End: 375s\n",
            "Start: 375s, End: 380s\n",
            "Start: 380s, End: 385s\n",
            "Start: 385s, End: 390s\n",
            "Key audio moments have been saved to 'key_audio_moments.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dont run this"
      ],
      "metadata": {
        "id": "YRoEzh3lbnra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Function to split audio into segments\n",
        "def split_audio_into_segments(audio_file, segment_length=10):\n",
        "    \"\"\"Splits audio file into segments of specified length.\"\"\"\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    segment_samples = segment_length * sr\n",
        "    num_segments = len(y) // segment_samples\n",
        "    segments = [y[i * segment_samples:(i + 1) * segment_samples] for i in range(num_segments)]\n",
        "    return segments, sr\n",
        "\n",
        "# Function to extract features from an audio file\n",
        "def extract_features(file, sr):\n",
        "    audio, _ = librosa.load(file, sr=sr)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "    return mfcc, mel, chroma, spectral_contrast\n",
        "\n",
        "# Process movies and trailers and extract features\n",
        "def process_movies_and_trailers(movie_files, trailer_files, sr):\n",
        "    all_mfcc_movie = []\n",
        "    all_mel_movie = []\n",
        "    all_chroma_movie = []\n",
        "    all_spectral_contrast_movie = []\n",
        "\n",
        "    all_mfcc_trailer = []\n",
        "    all_mel_trailer = []\n",
        "    all_chroma_trailer = []\n",
        "    all_spectral_contrast_trailer = []\n",
        "\n",
        "    for file in movie_files:\n",
        "        mfcc, mel, chroma, spectral_contrast = extract_features(file, sr)\n",
        "        all_mfcc_movie.append(mfcc.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_mel_movie.append(mel.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_chroma_movie.append(chroma.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_spectral_contrast_movie.append(spectral_contrast.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "\n",
        "    for file in trailer_files:\n",
        "        mfcc, mel, chroma, spectral_contrast = extract_features(file, sr)\n",
        "        all_mfcc_trailer.append(mfcc.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_mel_trailer.append(mel.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_chroma_trailer.append(chroma.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "        all_spectral_contrast_trailer.append(spectral_contrast.reshape(-1, 1))  # Reshape to (n_features, 1)\n",
        "\n",
        "    return all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie, \\\n",
        "           all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer\n",
        "\n",
        "# Prepare triplet data for Siamese LSTM\n",
        "def prepare_triplet_data(all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie,\n",
        "                         all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer):\n",
        "    \"\"\"Prepares triplet data for Siamese LSTM model.\"\"\"\n",
        "    anchors, positives, labels = [], [], []\n",
        "    num_movies = len(all_mfcc_movie)\n",
        "    num_trailers = len(all_mfcc_trailer)\n",
        "\n",
        "    for i in range(num_movies):\n",
        "        min_len = min(len(all_mfcc_movie[i]), len(all_mel_movie[i]), len(all_chroma_movie[i]),\n",
        "                      len(all_spectral_contrast_movie[i]))\n",
        "        for j in range(min_len):\n",
        "            combined_anchor = np.concatenate([all_mfcc_movie[i][j], all_mel_movie[i][j],\n",
        "                                              all_chroma_movie[i][j], all_spectral_contrast_movie[i][j]], axis=0)\n",
        "            trailer_idx = j % num_trailers  # Use modulo operator to wrap around\n",
        "            min_len_trailer = min(len(all_mfcc_trailer[trailer_idx]), len(all_mel_trailer[trailer_idx]),\n",
        "                                  len(all_chroma_trailer[trailer_idx]), len(all_spectral_contrast_trailer[trailer_idx]))\n",
        "            combined_positive = np.concatenate([all_mfcc_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_mel_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_chroma_trailer[trailer_idx][j % min_len_trailer],\n",
        "                                                all_spectral_contrast_trailer[trailer_idx][j % min_len_trailer]], axis=0)\n",
        "            anchors.append(combined_anchor)\n",
        "            positives.append(combined_positive)\n",
        "            labels.append(1)  # Positive pair\n",
        "\n",
        "    for i in range(num_trailers):\n",
        "        min_len = min(len(all_mfcc_trailer[i]), len(all_mel_trailer[i]), len(all_chroma_trailer[i]),\n",
        "                      len(all_spectral_contrast_trailer[i]))\n",
        "        for j in range(min_len):\n",
        "            combined_anchor = np.concatenate([all_mfcc_trailer[i][j], all_mel_trailer[i][j], all_chroma_trailer[i][j],\n",
        "                                              all_spectral_contrast_trailer[i][j]], axis=0)\n",
        "            movie_idx = j % num_movies  # Use modulo operator to wrap around\n",
        "            min_len_movie = min(len(all_mfcc_movie[movie_idx]), len(all_mel_movie[movie_idx]), len(all_chroma_movie[movie_idx]),\n",
        "                                len(all_spectral_contrast_movie[movie_idx]))\n",
        "            combined_positive = np.concatenate([all_mfcc_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_mel_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_chroma_movie[movie_idx][j % min_len_movie],\n",
        "                                                all_spectral_contrast_movie[movie_idx][j % min_len_movie]], axis=0)\n",
        "            anchors.append(combined_anchor)\n",
        "            positives.append(combined_positive)\n",
        "            labels.append(0)  # Negative pair\n",
        "\n",
        "    anchors = np.array(anchors)\n",
        "    positives = np.array(positives)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Standardize features for better training performance\n",
        "    scaler = StandardScaler()\n",
        "    anchors = scaler.fit_transform(anchors)\n",
        "    positives = scaler.transform(positives)\n",
        "\n",
        "    return anchors, positives, labels\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def compute_distance(tensors):\n",
        "    # Compute the squared differences\n",
        "    squared_diff = tf.square(tensors[0] - tensors[1])\n",
        "    # Sum across the feature dimension (axis=-1)\n",
        "    sum_squared_diff = tf.reduce_sum(squared_diff, axis=-1, keepdims=True)\n",
        "    # Take the square root\n",
        "    return tf.sqrt(sum_squared_diff)\n",
        "\n",
        "# Define the Siamese LSTM model architecture\n",
        "def create_siamese_lstm_model(input_shape):\n",
        "    \"\"\"Defines the Siamese LSTM model architecture.\"\"\"\n",
        "    timesteps, features = input_shape\n",
        "\n",
        "    input_a = Input(shape=(timesteps, features))  # Shape should be (timesteps, features)\n",
        "    input_b = Input(shape=(timesteps, features))  # Shape should be (timesteps, features)\n",
        "\n",
        "    # Shared LSTM layers\n",
        "    lstm = LSTM(128, return_sequences=True)\n",
        "    lstm_a = lstm(input_a)\n",
        "    lstm_a = LSTM(64)(lstm_a)  # LSTM for first input\n",
        "    dense_a = Dense(32, activation='relu')(lstm_a)\n",
        "\n",
        "    lstm_b = lstm(input_b)\n",
        "    lstm_b = LSTM(64)(lstm_b)  # LSTM for second input\n",
        "    dense_b = Dense(32, activation='relu')(lstm_b)\n",
        "\n",
        "    # Specify output shape correctly\n",
        "    distance = Lambda(compute_distance, output_shape=(1,))([dense_a, dense_b])\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
        "    return model\n",
        "\n",
        "# Contrastive loss function\n",
        "def contrastive_loss(margin=1):\n",
        "    def loss(y_true, y_pred):\n",
        "        # Add a small value to y_pred to prevent division by zero\n",
        "        y_pred = tf.maximum(y_pred, 1e-8)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = tf.reduce_mean((1 - y_true) * tf.square(y_pred) + y_true * tf.square(tf.maximum(margin - y_pred, 0)))\n",
        "\n",
        "        # Check for NaN values and replace them with zeros\n",
        "        loss = tf.where(tf.math.is_nan(loss), tf.zeros_like(loss), loss)\n",
        "\n",
        "        return loss\n",
        "    return loss\n",
        "\n",
        "# Example usage with movie and trailer files\n",
        "trailer_files = [\n",
        "        '/content/Stucco_Trailer.wav',\n",
        "        '/content/SushiNoh_Trailer.wav',\n",
        "        '/content/THECHAIR_Trailer.wav',\n",
        "        '/content/TheCouch_Trailer.wav',\n",
        "        '/content/TheElevator_Trailer.wav'\n",
        "]\n",
        "\n",
        "movie_files = [\n",
        "        '/content/Stucco _Movie.wav',\n",
        "        '/content/SushiNoh_Movie.wav',\n",
        "        '/content/THECHAIR_Movie.wav',\n",
        "        '/content/TheCouch_Movie.wav',\n",
        "        '/content/TheElevator_Movie.wav'\n",
        "]\n",
        "\n",
        "# Split audio into segments and process features\n",
        "segments, sr = split_audio_into_segments(trailer_files[0])\n",
        "all_mfcc_movie, all_mel_movie, all_chroma_movie, all_spectral_contrast_movie, \\\n",
        "all_mfcc_trailer, all_mel_trailer, all_chroma_trailer, all_spectral_contrast_trailer =\n",
        "                              process_movies_and_trailers(movie_files, trailer_files, sr)\n",
        "\n",
        "# Prepare triplet data\n",
        "anchors, positives, labels = prepare_triplet_data(all_mfcc_movie, all_mel_movie, all_chroma_movie,\n",
        "                                                all_spectral_contrast_movie, all_mfcc_trailer,\n",
        "                                                all_mel_trailer, all_chroma_trailer,\n",
        "                                                  all_spectral_contrast_trailer)\n",
        "\n",
        "# Reshape the anchors and positives for the LSTM input (batch_size, timesteps, features)\n",
        "batch_size = anchors.shape[0]\n",
        "time_steps = anchors.shape[1] // 4  # assuming 4 features\n",
        "anchors = anchors.reshape(batch_size, time_steps, 4)\n",
        "positives = positives.reshape(batch_size, time_steps, 4)\n",
        "\n",
        "# Create the Siamese LSTM model\n",
        "input_shape = anchors.shape[1:]  # (timesteps, 4)\n",
        "model = create_siamese_lstm_model(input_shape)\n",
        "\n",
        "# Add dropout to prevent overfitting\n",
        "x = model.output\n",
        "x = Dropout(0.2)(x)\n",
        "model = Model(inputs=model.input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001, clipvalue=0.5)\n",
        "model.compile(optimizer=optimizer, loss=contrastive_loss(margin=1))\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([anchors, positives], labels, epochs=30, batch_size=64,\n",
        "                    validation_split=0.2, callbacks=[reduce_lr, early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS45wtbhgWPU",
        "outputId": "d4d19f8f-93c4-40aa-e681-dfaef086fc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 11ms/step - loss: 0.2163 - val_loss: 0.9036 - learning_rate: 1.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 10ms/step - loss: 0.1997 - val_loss: 0.9036 - learning_rate: 1.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m47083/47083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 10ms/step - loss: 0.1997 - val_loss: 0.9036 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuWq47Ib-nlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NEW CODE"
      ],
      "metadata": {
        "id": "Lq3iceels0_E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rc3mWHjcs0G8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}